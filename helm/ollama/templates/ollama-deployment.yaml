apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ollama.fullname" . }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "ollama.labels" . | nindent 4 }}
    app.kubernetes.io/component: ollama
spec:
  replicas: {{ .Values.replicaCount }}
  strategy:
    type: Recreate
  selector:
    matchLabels:
      {{- include "ollama.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: ollama
  template:
    metadata:
      labels:
        {{- include "ollama.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: ollama
    spec:
      containers:
        - name: ollama
          image: "{{ .Values.ollama.image.repository }}:{{ .Values.ollama.image.tag }}"
          imagePullPolicy: {{ .Values.ollama.image.pullPolicy }}
          # Start server, then pull the model (no-op if already cached in the PVC).
          command: ["/bin/sh", "-c"]
          args:
            - |
              ollama serve &
              SERVER_PID=$!
              until ollama list > /dev/null 2>&1; do sleep 2; done
              ollama pull {{ .Values.ollama.model }}
              wait $SERVER_PID
          ports:
            - name: http
              containerPort: 11434
              protocol: TCP
          env:
            - name: OLLAMA_MODELS
              value: /models
            {{- if .Values.ollama.gpu.enabled }}
            - name: HSA_OVERRIDE_GFX_VERSION
              value: {{ .Values.ollama.gpu.hsaOverrideGfxVersion | quote }}
            {{- end }}
          readinessProbe:
            httpGet:
              path: /api/tags
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 30
          livenessProbe:
            httpGet:
              path: /api/tags
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            failureThreshold: 5
          resources:
            {{- if .Values.ollama.gpu.enabled }}
            requests:
              cpu: "1"
              memory: 2Gi
              amd.com/gpu: "1"
            limits:
              cpu: "4"
              memory: 4Gi
              amd.com/gpu: "1"
            {{- else }}
            {{- toYaml .Values.ollama.resources | nindent 12 }}
            {{- end }}
          volumeMounts:
            - name: models
              mountPath: /models
      {{- if .Values.ollama.gpu.enabled }}
      nodeSelector:
        amd.com/gpu.present: "true"
      {{- end }}
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: {{ include "ollama.fullname" . }}-models
